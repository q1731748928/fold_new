import requests
from bs4 import BeautifulSoup
import pandas as pd
import pymysql
from datetime import datetime
import random
import re
import time

# ==== å…¨å±€é…ç½® ====
headers = {
    'User-Agent': random.choice([
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0'
    ]),
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br'
}

hot_keywords = ["äººå·¥æ™ºèƒ½", "ç»æµ", "å…³ç¨"]  # å¯æ‰©å±•å…³é”®è¯
all_news = []  # å­˜å‚¨æ–°é—»æ•°æ®

# ==== é€šç”¨å·¥å…·å‡½æ•° ====
def normalize_time(raw_time):
    """æ ‡å‡†åŒ–æ—¶é—´æ ¼å¼"""
    try:
        # æ›¿æ¢ä¸­æ–‡æ—¥æœŸå•ä½
        normalized = raw_time.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', ' ').replace('æ—¶', ':').replace('åˆ†', '')
        # æå–æ—¶é—´æˆåˆ†
        time_match = re.search(
            r'(?P<year>\d{4})[-/]?(?P<month>\d{1,2})[-/]?(?P<day>\d{1,2})[^\d]*(?P<hour>\d{1,2}):?(?P<minute>\d{2})?',
            normalized
        )
        if time_match:
            components = time_match.groupdict()
            # è¡¥å…¨é»˜è®¤å€¼
            components['minute'] = components['minute'] or '00'
            return f"{components['year']}-{components['month']:0>2}-{components['day']:0>2} " \
                   f"{components['hour']:0>2}:{components['minute']:0>2}"
        return raw_time.strip()[:16]  # ä¿ç•™åŸå§‹æ ¼å¼å‰16å­—ç¬¦
    except:
        return "æœªçŸ¥"

# ==== ç½‘æ˜“æ–°é—»æŠ“å– ====
def fetch_163_news(keyword):
    print(f"ğŸ” æ­£åœ¨æŠ“å–ç½‘æ˜“æ–°é—»ï¼šå…³é”®è¯ã€{keyword}ã€‘")
    base_url = "https://news.163.com/"
    try:
        resp = requests.get(base_url, headers=headers, timeout=15)
        soup = BeautifulSoup(resp.text, 'lxml')

        for a in soup.find_all("a", href=True):
            title = a.get_text(strip=True)
            href = a['href']
            if keyword in title and href.startswith("http") and "163.com" in href:
                try:
                    # åŠ¨æ€è¯·æ±‚å¤´
                    article_headers = {
                        **headers,
                        'Referer': base_url,
                        'Sec-Fetch-Dest': 'document'
                    }
                    
                    # å¸¦é‡è¯•æœºåˆ¶çš„è¯·æ±‚
                    article = requests.get(href, headers=article_headers, timeout=20)
                    article.encoding = 'utf-8'
                    article_soup = BeautifulSoup(article.text, 'lxml')

                    # æ”¹è¿›çš„æ—¶é—´æå–
                    time_tag = article_soup.find("div", class_="post_time_source") or \
                              article_soup.find("span", class_="time")
                    pub_time = "æœªçŸ¥"
                    if time_tag:
                        raw_time = re.sub(r'\s+', ' ', time_tag.text.strip())
                        pub_time = normalize_time(raw_time.split('æ¥æº:')[0])

                    # å¢å¼ºçš„æ­£æ–‡æå–
                    content_div = article_soup.find("div", class_="post_body") or \
                                 article_soup.find("div", {"id": "content"})
                    
                    full_text = "æ— æ­£æ–‡å†…å®¹"
                    if content_div:
                        # æ¸…ç†å¹¿å‘Šå’Œå¹²æ‰°å…ƒç´ 
                        [ad.decompose() for ad in content_div.select(".gg200x300, .ep-source, .ad-wrap, script")]
                        paragraphs = content_div.select("p:not([class]):not([style])")
                        filtered_paragraphs = [p.get_text(strip=True) for p in paragraphs if len(p.text.strip()) > 15]
                        full_text = "\n".join(filtered_paragraphs) if filtered_paragraphs else "æ— æœ‰æ•ˆæ­£æ–‡å†…å®¹"

                    all_news.append({
                        "ç½‘ç«™": "ç½‘æ˜“æ–°é—»",
                        "æ ‡é¢˜": title[:200],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                        "é“¾æ¥": href,
                        "å…³é”®è¯": keyword,
                        "æŠ“å–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "å‘å¸ƒæ—¶é—´": pub_time,
                        "æ­£æ–‡": full_text[:5000]  # é™åˆ¶æ­£æ–‡é•¿åº¦
                    })

                except Exception as e:
                    print(f"[å­é”™è¯¯] ç½‘æ˜“è§£æå¤±è´¥ï¼š{str(e)[:60]}...")
    except Exception as e:
        print(f"[é”™è¯¯] æŠ“å–ç½‘æ˜“å¤±è´¥ï¼š{str(e)[:60]}...")

# ==== å¤®è§†æ–°é—»æŠ“å– ====
def fetch_sina_news(keyword):
    print(f"ğŸ” æ­£åœ¨æŠ“å–å¤®è§†æ–°é—»ï¼šå…³é”®è¯ã€{keyword}ã€‘")
    base_url = "https://news.cctv.com/china/"
    params = {
        'q': keyword,
        'c': 'news',
        'range': 'title',
        'num': 20,
        'col': '1_7',
        '_': str(int(time.time() * 1000)),  # é˜²ç¼“å­˜
        'callback': 'jsonp_callback'
    }
    
    try:
        resp = requests.get(base_url, params=params, headers=headers, timeout=20)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'lxml')

        news_found = False
        
        for item in soup.select('.result'):
            try:
                title_tag = item.select_one('h2 a')
                if not title_tag:
                    continue
                
                title = title_tag.text.strip()
                href = title_tag['href']
                
                # å¤šå±‚æ—¶é—´æå–
                time_container = item.select_one('.fgray, .source, .time-source')
                pub_time = "æœªçŸ¥"
                if time_container:
                    time_text = ' '.join(time_container.stripped_strings)
                    pub_time = normalize_time(time_text)

                # åŠ¨æ€è¯·æ±‚å¤´
                article_headers = {
                    **headers,
                    'Referer': base_url,
                    'Upgrade-Insecure-Requests': '1'
                }
                
                # å¸¦è¶…æ—¶é‡è¯•çš„è¯·æ±‚
                article = requests.get(href, headers=article_headers, timeout=(5, 20))
                article.encoding = article.apparent_encoding
                article_soup = BeautifulSoup(article.text, 'lxml')
                
                # å¤šç­–ç•¥æ­£æ–‡æå–
                content_div = article_soup.select_one('.article-content') or \
                             article_soup.find('div', {'id': 'artibody'}) or \
                             article_soup.find('div', {'class': 'article'})
                
                full_text = "æ— æ­£æ–‡å†…å®¹"
                if content_div:
                    # æ¸…ç†å¹²æ‰°å†…å®¹
                    [s.decompose() for s in content_div.select('.hidden, .article-editor, .article-keywords, .comment, iframe')]
                    paragraphs = content_div.select('p:not([style*="display:none"]):not(.article-keywords)')
                    valid_paragraphs = [p.text.strip() for p in paragraphs if 15 < len(p.text.strip()) < 500]
                    full_text = '\n'.join(valid_paragraphs) if valid_paragraphs else "æ— æœ‰æ•ˆæ­£æ–‡å†…å®¹"

                all_news.append({
                    "ç½‘ç«™": "æ–°æµªæ–°é—»",
                    "æ ‡é¢˜": title[:200],
                    "é“¾æ¥": href,
                    "å…³é”®è¯": keyword,
                    "æŠ“å–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "å‘å¸ƒæ—¶é—´": pub_time,
                    "æ­£æ–‡": full_text[:5000]
                })
                news_found = True

            except Exception as e:
                print(f"[å­é”™è¯¯] å¤®è§†è§£æå¤±è´¥ï¼š{str(e)[:60]}...")

        if not news_found:
            print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¤®è§†æ–°é—»ï¼šå…³é”®è¯ã€{keyword}ã€‘")

    except Exception as e:
        print(f"[é”™è¯¯] æŠ“å–å¤®è§†å¤±è´¥ï¼š{str(e)[:60]}...")

# ==== æ•°æ®å­˜å‚¨ ====
def save_to_mysql(data_list):
    try:
        conn = pymysql.connect(
            host='localhost',
            user='root',
            password='xzxz083083',
            database='news_db',
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )
        
        with conn.cursor() as cursor:
            # åˆ›å»ºè¡¨ï¼ˆå¸¦å»é‡ç´¢å¼•ï¼‰
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS hot_news (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    ç½‘ç«™ VARCHAR(100) NOT NULL,
                    æ ‡é¢˜ VARCHAR(200) NOT NULL,
                    é“¾æ¥ VARCHAR(500) UNIQUE,  -- é“¾æ¥å”¯ä¸€ç´¢å¼•
                    å…³é”®è¯ VARCHAR(50) NOT NULL,
                    æŠ“å–æ—¶é—´ DATETIME NOT NULL,
                    å‘å¸ƒæ—¶é—´ VARCHAR(20),
                    æ­£æ–‡ TEXT,
                    INDEX idx_keyword (å…³é”®è¯),
                    INDEX idx_source (ç½‘ç«™)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
            """)
            
            # æ‰¹é‡æ’å…¥ï¼ˆå¿½ç•¥é‡å¤é“¾æ¥ï¼‰
            sql = """INSERT IGNORE INTO hot_news 
                    (ç½‘ç«™, æ ‡é¢˜, é“¾æ¥, å…³é”®è¯, æŠ“å–æ—¶é—´, å‘å¸ƒæ—¶é—´, æ­£æ–‡)
                    VALUES (%s, %s, %s, %s, %s, %s, %s)"""
            batch_data = [
                (item['ç½‘ç«™'], item['æ ‡é¢˜'], item['é“¾æ¥'], item['å…³é”®è¯'],
                 item['æŠ“å–æ—¶é—´'], item['å‘å¸ƒæ—¶é—´'], item['æ­£æ–‡'])
                for item in data_list
            ]
            cursor.executemany(sql, batch_data)
            
        conn.commit()
        print(f"âœ… æˆåŠŸæ’å…¥ {cursor.rowcount} æ¡æ•°æ®åˆ°MySQL")

    except Exception as e:
        print(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥ï¼š{str(e)[:200]}")
    finally:
        if conn:
            conn.close()

# ==== ä¸»ç¨‹åº ====
if __name__ == "__main__":
    # æ‰§è¡ŒæŠ“å–ä»»åŠ¡
    for keyword in hot_keywords:
        fetch_sina_news(keyword)
        fetch_163_news(keyword)
        time.sleep(random.uniform(1, 3))  # éšæœºå»¶è¿Ÿé˜²å°ç¦

    # æ•°æ®å¤„ç†
    if not all_news:
        print("âš ï¸ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ–°é—»")
    else:
        # æ•°æ®æ¸…æ´—
        df = (
            pd.DataFrame(all_news)
            .drop_duplicates(subset=['é“¾æ¥'], keep='last')
            .sort_values('å‘å¸ƒæ—¶é—´', ascending=False)
        )
        
        # æ–‡ä»¶å­˜å‚¨
        df.to_csv("hot_news.csv", index=False, encoding="utf-8-sig")
        df.to_json("hot_news.json", orient="records", force_ascii=False, indent=2)
        print(f"ğŸ“ å·²ä¿å­˜ {len(df)} CSV.JSON")
        
        # æ•°æ®åº“å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
        #if not df.empty:
           #save_to_mysql(df.to_dict('records'))
