{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import urllib.parse\n",
    "\n",
    "# 设置请求头模拟浏览器访问\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'zh-CN,zh;q=0.9',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Referer': 'https://travel.qunar.com/'\n",
    "}\n",
    "\n",
    "def get_guide_urls(page_num):\n",
    "    url = f\"https://travel.qunar.com/travelbook/list.htm?page={page_num}&order=hot_heat\"\n",
    "    try:\n",
    "        print(f\"Requesting URL: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.encoding = 'utf-8'  # 确保正确编码\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to get page {page_num}, status code: {response.status_code}\")\n",
    "            return []\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # 尝试多种可能的选择器\n",
    "        guide_list = soup.find('ul', class_='b_strategy_list')\n",
    "        if not guide_list:\n",
    "            print(\"Couldn't find 'b_strategy_list', trying alternative selectors\")\n",
    "            guide_list = soup.find('ul', class_='list_item')\n",
    "        \n",
    "        if not guide_list:\n",
    "            print(f\"No guide list found on page {page_num} with any selector\")\n",
    "            return []\n",
    "            \n",
    "        guides = guide_list.find_all('li')\n",
    "        urls = []\n",
    "        \n",
    "        for guide in guides:\n",
    "            # 尝试多种可能的选择器找到链接\n",
    "            a_tag = guide.find('a', class_='tit')\n",
    "            if not a_tag:\n",
    "                a_tag = guide.find('a', class_='title')\n",
    "            if not a_tag:\n",
    "                a_tag = guide.find('h2').find('a') if guide.find('h2') else None\n",
    "                \n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                guide_url = urllib.parse.urljoin('https://travel.qunar.com', a_tag['href'])\n",
    "                urls.append(guide_url)\n",
    "                print(f\"Found guide: {a_tag.text.strip() if a_tag.text else 'No title'} - {guide_url}\")\n",
    "        \n",
    "        print(f\"Found {len(urls)} guides on page {page_num}\")\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting guide URLs from page {page_num}: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_guide_data(url):\n",
    "    try:\n",
    "        print(f\"Extracting data from: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.encoding = 'utf-8'  # 确保正确编码\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to get {url}, status code: {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        data = {}\n",
    "        \n",
    "        # 攻略标题 - 尝试多种选择器\n",
    "        title_elem = soup.find('h1', class_='booktitle')\n",
    "        if not title_elem:\n",
    "            title_elem = soup.find('h1', class_='title')\n",
    "        if not title_elem:\n",
    "            title_elem = soup.find('h1')\n",
    "        data['title'] = title_elem.text.strip() if title_elem else None\n",
    "        \n",
    "        # 用户名 - 尝试多种选择器\n",
    "        author_elem = soup.find('span', class_='user_name')\n",
    "        if not author_elem:\n",
    "            author_elem = soup.find('a', class_='author_name')\n",
    "        if not author_elem:\n",
    "            author_info = soup.find('div', class_='c_author_name') or soup.find('div', class_='author')\n",
    "            author_elem = author_info.find('a') if author_info else None\n",
    "        data['username'] = author_elem.text.strip() if author_elem else None\n",
    "        \n",
    "        # 途径路线\n",
    "        route_elem = soup.find('div', class_='places')\n",
    "        if not route_elem:\n",
    "            route_elem = soup.find('div', class_='route')\n",
    "        if not route_elem:\n",
    "            route_info = soup.find('div', {'data-reactid': re.compile(r'.*places.*')})\n",
    "            route_elem = route_info if route_info else None\n",
    "        data['route'] = route_elem.text.strip() if route_elem else None\n",
    "        \n",
    "        # 行程内容\n",
    "        content_div = soup.find('div', class_='b_panel_content')\n",
    "        if not content_div:\n",
    "            content_div = soup.find('div', class_='content')\n",
    "        if not content_div:\n",
    "            content_div = soup.find('div', class_='b_notes_content')\n",
    "        data['itinerary'] = content_div.text[:200].strip() if content_div else None\n",
    "        \n",
    "        # 出发日期\n",
    "        date_pattern = re.compile('出发时间|出行时间|旅行时间')\n",
    "        date_elem = None\n",
    "        for elem in soup.find_all('li', class_='item'):\n",
    "            if elem.text and date_pattern.search(elem.text):\n",
    "                date_elem = elem\n",
    "                break\n",
    "        \n",
    "        if date_elem:\n",
    "            value_span = date_elem.find('span', class_='value')\n",
    "            data['departure_date'] = value_span.text.strip() if value_span else date_elem.text.replace('出发时间', '').replace('出行时间', '').replace('旅行时间', '').strip()\n",
    "        else:\n",
    "            date_div = soup.find('div', text=date_pattern)\n",
    "            if date_div:\n",
    "                next_elem = date_div.find_next()\n",
    "                data['departure_date'] = next_elem.text.strip() if next_elem else None\n",
    "            else:\n",
    "                data['departure_date'] = None\n",
    "        \n",
    "        # 行程天数\n",
    "        days_pattern = re.compile('出行天数|行程天数|旅行天数')\n",
    "        days_elem = None\n",
    "        for elem in soup.find_all('li', class_='item'):\n",
    "            if elem.text and days_pattern.search(elem.text):\n",
    "                days_elem = elem\n",
    "                break\n",
    "                \n",
    "        if days_elem:\n",
    "            value_span = days_elem.find('span', class_='value')\n",
    "            data['days'] = value_span.text.strip() if value_span else days_elem.text.replace('出行天数', '').replace('行程天数', '').replace('旅行天数', '').strip()\n",
    "        else:\n",
    "            days_div = soup.find('div', text=days_pattern)\n",
    "            if days_div:\n",
    "                next_elem = days_div.find_next()\n",
    "                data['days'] = next_elem.text.strip() if next_elem else None\n",
    "            else:\n",
    "                # 尝试从标题或内容中提取天数\n",
    "                days_match = re.search(r'\\d+[天日]', data['title'] if data['title'] else '')\n",
    "                if not days_match and data['itinerary']:\n",
    "                    days_match = re.search(r'\\d+[天日]', data['itinerary'])\n",
    "                data['days'] = days_match.group(0) if days_match else None\n",
    "        \n",
    "        # 人物标签和玩法标签\n",
    "        tags_div = soup.find('div', class_='tags')\n",
    "        if not tags_div:\n",
    "            # 尝试找到所有可能的标签元素\n",
    "            tag_elems = soup.find_all('span', class_='tag') or soup.find_all('a', class_='tag')\n",
    "            if tag_elems:\n",
    "                tags = [tag.text.strip() for tag in tag_elems if tag.text.strip()]\n",
    "                data['tags'] = ', '.join(tags)\n",
    "            else:\n",
    "                data['tags'] = None\n",
    "        else:\n",
    "            data['tags'] = tags_div.text.strip()\n",
    "        \n",
    "        # 人均费用\n",
    "        cost_pattern = re.compile('人均花费|人均消费|人均开销|费用')\n",
    "        cost_elem = None\n",
    "        for elem in soup.find_all('li', class_='item'):\n",
    "            if elem.text and cost_pattern.search(elem.text):\n",
    "                cost_elem = elem\n",
    "                break\n",
    "                \n",
    "        if cost_elem:\n",
    "            value_span = cost_elem.find('span', class_='value')\n",
    "            data['cost'] = value_span.text.strip() if value_span else cost_elem.text.replace('人均花费', '').replace('人均消费', '').replace('人均开销', '').replace('费用', '').strip()\n",
    "        else:\n",
    "            cost_div = soup.find('div', text=cost_pattern)\n",
    "            if cost_div:\n",
    "                next_elem = cost_div.find_next()\n",
    "                data['cost'] = next_elem.text.strip() if next_elem else None\n",
    "            else:\n",
    "                # 尝试从内容中提取费用信息\n",
    "                if data['itinerary']:\n",
    "                    cost_match = re.search(r'\\d+元|\\d+\\.\\d+元|\\d+块|\\d+\\.\\d+块|\\d+人民币|\\d+\\.\\d+人民币', data['itinerary'])\n",
    "                    data['cost'] = cost_match.group(0) if cost_match else None\n",
    "                else:\n",
    "                    data['cost'] = None\n",
    "        \n",
    "        # 照片数量\n",
    "        all_imgs = soup.find_all('img')\n",
    "        content_imgs = []\n",
    "        if content_div:\n",
    "            content_imgs = content_div.find_all('img')\n",
    "        data['photo_count'] = len(content_imgs) if content_imgs else len(all_imgs)\n",
    "        \n",
    "        # 文字内容 (摘要)\n",
    "        if content_div:\n",
    "            text_content = content_div.text.strip()\n",
    "            # 移除过长的空白\n",
    "            text_content = re.sub(r'\\s+', ' ', text_content)\n",
    "            data['text_content'] = text_content[:300] if text_content else None\n",
    "        else:\n",
    "            data['text_content'] = None\n",
    "        \n",
    "        # 图片内容 (第一张照片的URL)\n",
    "        img_elem = None\n",
    "        if content_div:\n",
    "            img_elem = content_div.find('img')\n",
    "        if not img_elem:\n",
    "            img_elem = soup.find('img', class_='cover') or soup.find('img')\n",
    "            \n",
    "        if img_elem and 'src' in img_elem.attrs:\n",
    "            img_src = img_elem['src']\n",
    "            # 确保是完整URL\n",
    "            if not img_src.startswith('http'):\n",
    "                img_src = urllib.parse.urljoin('https://travel.qunar.com', img_src)\n",
    "            data['image_content'] = img_src\n",
    "        else:\n",
    "            data['image_content'] = None\n",
    "        \n",
    "        # 统计数据\n",
    "        # 观看数\n",
    "        view_pattern = re.compile('浏览|查看|阅读')\n",
    "        views_elem = soup.find('span', class_=lambda c: c and 'view' in c) or soup.find('span', text=view_pattern)\n",
    "        data['views'] = views_elem.text.strip() if views_elem else None\n",
    "        \n",
    "        # 点赞数\n",
    "        like_pattern = re.compile('赞|点赞|顶')\n",
    "        likes_elem = soup.find('span', class_=lambda c: c and ('like' in c or 'ding' in c)) or soup.find('span', text=like_pattern)\n",
    "        data['likes'] = likes_elem.text.strip() if likes_elem else None\n",
    "        \n",
    "        # 收藏数\n",
    "        fav_pattern = re.compile('收藏|珍藏')\n",
    "        favs_elem = soup.find('span', class_=lambda c: c and ('collect' in c or 'fav' in c)) or soup.find('span', text=fav_pattern)\n",
    "        data['favorites'] = favs_elem.text.strip() if favs_elem else None\n",
    "        \n",
    "        # 分享数\n",
    "        share_pattern = re.compile('分享')\n",
    "        share_elem = soup.find('span', class_=lambda c: c and 'share' in c) or soup.find('span', text=share_pattern)\n",
    "        data['shares'] = share_elem.text.strip() if share_elem else None\n",
    "        \n",
    "        # 评论数量和内容\n",
    "        comment_pattern = re.compile('评论')\n",
    "        comment_elem = soup.find('span', class_=lambda c: c and 'comment' in c) or soup.find('span', text=comment_pattern) \n",
    "        data['comments'] = comment_elem.text.strip() if comment_elem else None\n",
    "        \n",
    "        # 记录爬取成功的URL\n",
    "        data['url'] = url\n",
    "        \n",
    "        # 记录日志\n",
    "        print(f\"Successfully extracted data for: {data['title']}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# 主爬虫逻辑\n",
    "all_data = []\n",
    "max_pages = 50  # 增加页面数以确保获取足够数据\n",
    "min_data_required = 120  # 目标数据量\n",
    "\n",
    "for page in range(1, max_pages + 1):\n",
    "    print(f\"\\n{'='*50}\\nProcessing page {page}...\\n{'='*50}\")\n",
    "    urls = get_guide_urls(page)\n",
    "    \n",
    "    if not urls:\n",
    "        print(f\"No URLs found on page {page}, trying next page...\")\n",
    "        continue\n",
    "        \n",
    "    for url in urls:\n",
    "        try:\n",
    "            print(f\"\\nProcessing URL: {url}\")\n",
    "            data = extract_guide_data(url)\n",
    "            \n",
    "            if data:\n",
    "                all_data.append(data)\n",
    "                print(f\"Successfully extracted data. Total collected: {len(all_data)}\")\n",
    "            else:\n",
    "                print(f\"Failed to extract data from {url}\")\n",
    "                \n",
    "            # 随机延迟，避免请求过于频繁\n",
    "            delay = random.uniform(3, 7)\n",
    "            print(f\"Waiting for {delay:.2f} seconds...\")\n",
    "            time.sleep(delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "            \n",
    "        # 检查是否已收集足够数据\n",
    "        if len(all_data) >= min_data_required:\n",
    "            print(f\"Reached target data count: {len(all_data)}\")\n",
    "            break\n",
    "            \n",
    "    if len(all_data) >= min_data_required:\n",
    "        print(f\"Reached target data count: {len(all_data)}. Stopping.\")\n",
    "        break\n",
    "        \n",
    "    # 页面间延迟\n",
    "    page_delay = random.uniform(5, 10)\n",
    "    print(f\"Finished page {page}. Waiting {page_delay:.2f} seconds before next page...\")\n",
    "    time.sleep(page_delay)\n",
    "\n",
    "# 数据预处理\n",
    "print(f\"\\nData collection complete. Collected {len(all_data)} raw entries.\")\n",
    "print(\"Processing data...\")\n",
    "\n",
    "if all_data:\n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # 删除完全重复的行\n",
    "    initial_count = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {initial_count - len(df)} completely duplicate entries.\")\n",
    "    \n",
    "    # 删除标题和用户名重复的行\n",
    "    pre_dedup_count = len(df)\n",
    "    df = df.drop_duplicates(subset=['title', 'username'], keep='first')\n",
    "    print(f\"Removed {pre_dedup_count - len(df)} entries with duplicate titles and usernames.\")\n",
    "    \n",
    "    # 处理缺失值\n",
    "    print(\"\\nMissing value counts before filling:\")\n",
    "    for column in df.columns:\n",
    "        null_count = df[column].isna().sum()\n",
    "        if null_count > 0:\n",
    "            print(f\"Column '{column}' has {null_count} missing values.\")\n",
    "            \n",
    "    # 填充缺失值\n",
    "    df = df.fillna({\n",
    "        'title': 'Unknown Title',\n",
    "        'username': 'Anonymous User',\n",
    "        'route': 'Not Specified',\n",
    "        'itinerary': 'Not Available',\n",
    "        'departure_date': 'Unknown',\n",
    "        'days': 'Not Specified',\n",
    "        'tags': 'No Tags',\n",
    "        'cost': 'Not Specified',\n",
    "        'text_content': 'No Content',\n",
    "        'image_content': 'No Image',\n",
    "        'views': '0',\n",
    "        'likes': '0',\n",
    "        'favorites': '0',\n",
    "        'shares': '0',\n",
    "        'comments': '0'\n",
    "    })\n",
    "    \n",
    "    # 数值列处理 - 确保数值列为数值类型\n",
    "    # 从统计数据中提取数字\n",
    "    def extract_number(text):\n",
    "        if pd.isna(text):\n",
    "            return 0\n",
    "        # 提取数字部分\n",
    "        match = re.search(r'\\d+', str(text))\n",
    "        if match:\n",
    "            return int(match.group())\n",
    "        return 0\n",
    "    \n",
    "    # 处理统计类列\n",
    "    for col in ['views', 'likes', 'favorites', 'shares', 'comments']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(extract_number)\n",
    "    \n",
    "    # 确保photo_count为整数\n",
    "    if 'photo_count' in df.columns:\n",
    "        df['photo_count'] = pd.to_numeric(df['photo_count'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # 提取天数为数字\n",
    "    def extract_days(text):\n",
    "        if pd.isna(text):\n",
    "            return None\n",
    "        # 提取数字部分\n",
    "        match = re.search(r'\\d+', str(text))\n",
    "        if match:\n",
    "            return int(match.group())\n",
    "        return None\n",
    "    \n",
    "    if 'days' in df.columns:\n",
    "        df['trip_days'] = df['days'].apply(extract_days)\n",
    "        df['trip_days'] = pd.to_numeric(df['trip_days'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # 将数值型的列转换为整数\n",
    "    numeric_cols = ['photo_count', 'views', 'likes', 'favorites', 'shares', 'comments', 'trip_days']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(int)\n",
    "    \n",
    "    # 检查处理后的数据量\n",
    "    final_count = len(df)\n",
    "    print(f\"\\nFinal dataset contains {final_count} entries.\")\n",
    "    \n",
    "    if final_count < 100:\n",
    "        print(f\"Warning: Only collected {final_count} unique entries, which is less than the required 100.\")\n",
    "    else:\n",
    "        print(f\"Successfully collected {final_count} unique entries, meeting the requirement of at least 100 entries.\")\n",
    "    \n",
    "    # 保存到CSV文件\n",
    "    output_file = 'travel_guides.csv'\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    # 显示数据集的信息\n",
    "    print(\"\\nDataset information:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nSample data (first 5 rows):\")\n",
    "    print(df[['title', 'username', 'route', 'days', 'cost', 'photo_count', 'views', 'likes']].head())\n",
    "else:\n",
    "    print(\"No data collected. Please check the website structure or network connection and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试爬虫代码\n",
    "def test_parser(url=None):\n",
    "    if not url:\n",
    "        # 默认测试URL\n",
    "        url = \"https://travel.qunar.com/travelbook/note/7450340\"\n",
    "        \n",
    "    print(f\"Testing parser on URL: {url}\")\n",
    "    try:\n",
    "        data = extract_guide_data(url)\n",
    "        if data:\n",
    "            print(\"\\nExtracted data:\")\n",
    "            for key, value in data.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            return data\n",
    "        else:\n",
    "            print(\"Failed to extract data\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing parser: {e}\")\n",
    "        return None\n",
    "\n",
    "# 测试列表页解析\n",
    "def test_list_parser(page_num=1):\n",
    "    print(f\"Testing list page parser on page {page_num}\")\n",
    "    try:\n",
    "        urls = get_guide_urls(page_num)\n",
    "        print(f\"Found {len(urls)} URLs on page {page_num}\")\n",
    "        if urls:\n",
    "            for i, url in enumerate(urls[:5], 1):  # 只显示前5个URL\n",
    "                print(f\"{i}. {url}\")\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing list parser: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "test_list_parser(1)  # 测试第一页的列表解析\n",
    "test_parser()  # 测试单个详情页解析"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
